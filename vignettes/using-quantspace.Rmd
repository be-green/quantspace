---
title: "Using quantspace"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{using-quantspace}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6.5
)
```

The `quantspace` library implements a flexible approach to quantile estimation which guarentees that quantiles do not cross at any point in the data. This will walk through the basic usage of quantspace, how to change algorithms, use different standard error methods, handle plotting, prediction, and interpreting coefficients.

To start, we load the library:

```{r setup}
library(quantspace)
set.seed(1999)
```

To get a sense of how quantspace works, I'll use some simulated data.

```{r}
# Design Matrix
X <- matrix(rnorm(10000), ncol = 2)
betas <- c(-1, 2)

# make the scale also depend on X
scale_betas <- c(0.25, 1.5)
err <- exp(X %*% scale_betas) * rnorm(nrow(X))
y <- X %*% betas + err
```

This gives us some fatter tails and some dependence in the tails. Let's look at y in order to get a sense.

```{r plot_data, message = F, fig.width = 6.5}
library(ggplot2)

# what does the tail dependence do relative to normal errors
y_no_err_dep <- X %*% betas + rnorm(nrow(X))

qplot(y_no_err_dep) + ggtitle("Y with normal errors")
```

```{r}
qplot(y) + ggtitle("Y with tail dependence")
```

# The `qs` function

## Basic use
Let's try to get at that info! The main function which estimates models via quantile spacings is the `qs` function. Let's check out an example:

```{r my_first_qs}
regression_data <- data.frame(y = y, X)

# fits a quantile spacings model
# these quantiles are set as the default argument, and you
# can change them as you need to
fit <- qs(y ~ X1 + X2, data = regression_data, 
          quantiles = c(0.1, 0.25, 0.5, 0.75,0.9), 
          baseline_quantile = 0.5,
          algorithm = "sfn")

summary(fit)
```

We've already made a couple choices here. First, we set the regression formula, second we specified the data. The ones that are a bit stranger are the `quantiles` and `baseline_quantile` arguments. Because the quantile spacings model is a _joint_ model, we need to estimate everything recursively. 

To get the 0.25 spacing coefficient, we take the predicted median value, $\hat{y}_{med}$, and calculate $r = y - \hat{y}_{med}$. For the positive residuals, we then take `log(r)` as the outcome variable, and estimate a median regression. For negative residuals, we do the same thing but with `log(-r)`. This guarentees that the quantiles fitted by the model don't ever cross.

The `baseline_quantile` is the quantile that we take spacings relative to. We don't recommend changing this away from the median, which is the default. The `quantiles` argument is simply the complete list of quantile effects you want to estimate.

The `algorithm` argument is the algorithm you want to use in order to fit the quantile regression at each stage. This will be covered in more detail later, but the options include all the algorithms implemented in the `quantreg` package (e.g. "br", "pfn", "sfn") as well as "agd", which implements accelerated gradient descent over a semi-smooth approximation of the quantile loss function, and "lasso" and "post-lasso", which are penalized variants of the other algorithms. There will be more to come in the future, and there is an interface for adding your own algorithm which will be documented in a separate vignette. By default, this is "sfn", which is the fastest method available in the `quantreg` package for most problems. For large-scale problems, you may see speed gains from using the "agd" algorithm.

_Note (June 7, 2021): the interface to the penalized regression variants is likely to change in the near future. Treat as experimental._

There are also `s3` methods common to other packages available for basic operations. These include `coef` and `coefficients`, which return a matrix of coefficients, `predict` which returns fitted quantiles (or out of sample predictions via the argument `newdata`), `summary` which summarizes the fitted model, and `resid` and `residuals` which return model residuals.

```{r}
# get coefficients
betas <- coef(fit)

# get residuals
res <- resid(fit)

# in sample predictions
pred <- predict(fit)

# out of sample predictions
oos <- data.frame(matrix(rnorm(200), ncol = 2))
pred_oos <- predict(fit, newdata = oos)
```

The `qs` function also has some control parameters you can see described in more detail on the help page for the function (`?qs`), including `parallel`, which determines whether bootstrap draws should be parallelized, `calc_se` which determines whether to calculate standard errors, and `weights`, which allows you run a weighted regression.

## Advanced Usage

The more sophisticated controls for the quantile spacings model and standard errors are specified via the `control` and `std_err_control` arguments. Each of these is specified via calls to the `qs_control()` and `se_control()` functions. For details, you can always check the help pages (e.g. `?qs_control`).

The `qs_control` function allows you to set `small`, the level at which values are treated as zero, `trunc`, whether to truncate residuals below the value of `small`, `lambda`, an optional penalty parameter for use with the `lasso` algorithm, `output_quantiles`, which controls whether to save fitted quantiles, and `calc_avg_me`, which determines whether to calculate average marginal effects. Perhaps with the exception of `lambda`, none of these need to be changed very often (if at all).

The `se_control` function allows the user to specify various aspects of the bootstrapping and subsampling used to calculate standard errors. By default, standard errors are calculated via subsampling, with 100 draws. There are several available algorithms for computing standard errors, all based on empirical reweighting and bootstrapping. Setting `se_control(se_method = "weighted_bootstrap"`) will randomly re-weight data based on random exponential weights drawn with rate parameter 1. Using `"subsample"`, which is the default, will construct subsamples of the data (without replacement. The size of these subsamples is set via `subsample_percent`. Using `"bootstrap"` will use data sample with replacement the same size as the original sample. To truly customize this, you can set this argument to `"rq_sample"`.

The other `se_control` parameters allow you to customize the subsampling percent, the number of bootstraps (`num_bs`), whether to use random exponential weights (`draw_weights`), whether to sample with or without replacement or to leave the sample alone (`sampling_method`), and the size of the data at which point the bootstrapping/subsamplign will be done in parallel (`parallel_thresh`).

In the future, the package may add closed-form standard errors via the delta method, but as of now all standard error methods are based on bootstrapping and subsampling.

# Distributional Modeling

The other thing that the `quantspace` package is very useful for is modeling the full distribution of outcomes. To do this, it contains routines which efficiently interpolate the fitted quantiles via cubic splines. Modeling the log distance away from the median, rather than treating each quantile independently, prevents the fitted quantiles from crossing, which means that we can always recover a valid density by interpolating between them. 

Right now this is done with cubic splines, which can (very occasionally) go negative, which would not imply a valid CDF. If that happens, it's probably a sign that this is a very poor model for your problem or that you have data which is quite small.

So how do we do this? Let's take our fit from above and interpolate it. Because our model is non-linear, the distance between quantiles will vary with the data. By default, the `distributional_effects` function performs this interpolation at the average level of the data, returning the pdf, cdf, quantile function, and a random number generator for the fitted density.

```{r my_first_de}


d <- distributional_effects(fit)

list(
  # evaluate the density at 0
  pdf = d$pdf(0),
  # evaluate the cdf at 0
  cdf = d$cdf(0), 
  # return the fitted median
  quantile = d$q(0.5), 
  # generate 10 random numbers from fitted density
  d$r(10)
)

```

We can also do this for various levels of the data. Let's see how these change as we move up in the first dimension of our X variable.


```{r plot_de, fig.width= 6.5}

X_levels <- data.frame(quantiles = c(0, 0.25, 0.5, 0.75, 1),
                       X1 = mean(X[1,]), 
                       X2 = quantile(X[2,]))

# interpolate fitted quantiles at all levels of X1
vary_x2 <- distributional_effects(fit, newdata = X_levels, tails = "gaussian")

plot(vary_x2, tail_level = 0.001)
```
